
## [Constraints and Hallucinations: Filling in the Details](https://www.youtube.com/watch?v=mwxknB4SgvM)  
by Gerald Jay Sussman  
given at Google sometime in 2013  

### Abstract

Ever since I was an MIT undergraduate I have been pursuing several overlapping goals:

1. To formally understand the modeling and problem-solving strategies used by scientists and engineers.

2. To automate parts of this modeling and problem solving.

3. To devise linguistic support for expressing this how-to knowledge in a way that can be understood by a human reader as well as executed by a computer.

4. To use this computational medium to communicate how-to skills more effectively to students of science and engineering.

In pursuit of these goals I have been inspired by an idea that I gleaned from conversations with Jerry Lettvin about neurophysiology when I was an undergraduate.

I will advance the following cognitive-science hypothesis: Much of what we call intelligence is filling in details in pre-established networks of constraints. Indeed, I believe that most of the reason that humans are successful problem solvers is that we can often turn complex problems into finite networks of constraints that can be attacked with with a simple process called "propagation".

For propagation to be effective it will be important that the constraints and the data that they manipulate be finite and discrete, often "symbolic", expressions.

It will turn out that appropriately configured constraint networks can be a parallel computational architecture that can be used to track the provenance of data, explain the reasons for a belief, attribute blame for a failure, learn from mistakes, and allow a system to hold multiple locally-consistent worldviews in a globally-inconsistent "mind".
 
I will illustrate these ideas with techniques for formalizing some problem-solving strategies. It will be necessary to interweave yet separate the results of a computation and the strategy by which it is accomplished. Some deductions are strategic deductions and some are actual result deductions. To effectively represent strategies it will be necessary for propositions to be able to refer to other propositions. For example, rules will have to be able to be triggered in the case that the belief status of a proposition is unknown.

The propagation model of computation supports an appropriate infrastructure, a kind of plumbing for building such systems.

### Introduction (by [Chris Hanson](https://people.csail.mit.edu/cph/))

This is Gerald Jay Sussman, my mentor and good friend, who has probably taught at least a few people in here. He is one of Marvin Minsky's early students, has done a lot of interesting things in his life and he is going to tell us about the latest stuff he has been thinking about. 

### Halucinations are necessary in order to perceive

One thing I am going to start with is very old stuff. When I was an undergraduate, which was a long time ago (64-68 at MIT), there was a fellow named Jerome Lettvin, who was a very interesting character. I would go to talk to him occasionally. He had ideas about neurophysiology which were mostly very good and sometimes wrong, but it did not matter cause his stories were so good that it did not matter whether they were right or wrong. They could have been good ways of thinking for lots of things. One of the things I learned from Mr. Lettvin was that almost none of what we perceive is the stuff that is out there. That in fact we make up most of it. And in fact it's essential from his point of view that we make up the content. So I want to talk about filling in details and that basically hallucinations are necessary in order to perceive. Then I want to show how that leads to invention of a kind of system for doing reasoning which is not unusual but which mirrors that way of the working with the world. 

### [Cognition involves filling in details](slides/cognition-involves-filling-in-details.png)

One of the observations is that in order to see or perceive things you better fill in details, there are details that come from everywhere, that in fact is essential to halucinate details if they are not there and it happens very fast. Look at this [picture](slides/cognition-involves-filling-in-details.png). Everybody sees these 2 scenes. What is this? - a car. What is this? These are the same bit patterns, just flipped. You created the scene that was not there. These are the actual bits. There is a wheel, it is all there. The same bits. 

### [Kanizsa's Triangle Illusion](slides/kanizsa-triangle.png)

Do you see the triangle that is not there? Everybody knows that isn't there, however you see it very well. This is an important observation. Btw., we know that bees do that also. They can see Kanizsa's triangle, but pigeons do not. This is sort of a wiring thing. Bees have been tested. You train bunch of bees to go to a place where there is sugar water and one of these images (with a triangle), and you move that around, and eventually you see that bees go to the Kanizsa's triangle. Well, there are clues here, there are local clues. 

### [Cognition fills in details](slides/cognition-fills-in-details.png)

[David Waltz](https://en.wikipedia.org/wiki/David_Waltz), great wizard of when I was a graduate student, invented a mechanism of being able to label line drawings so that you can imagine that there a 3D objects. If you look at [this](slides/cognition-fills-in-details.png), this a line that is sticking out at you, "-" is inside, so this is some kind of depression in a surface. This means it is an open area, that behind this is nothing, so this is a hole that goes all the way through. For this one over here, we are looking at something that is floating in space and is coming up which is like truncated pyramid. This one is on the table and is a truncated pyramid. These are little programs which Waltz wrote, which were beautiful, which could take a bunch of lines and produce these labelings. The way they did it is quite clear. In this (lower right case), all possible labelings of that corner are known - there is some kind of catalog of them. All possible labelings of any corner are known. There has to be consistency. When lines come together, they have to be the same label, and that is the whole thing. It is very much like with the Kanizsa's triangle, that local evidence is put together to make a global interpretation. 

### [Circuit Analysis is filling in details](slides/circuit-analysis-is-filling-in-details.png)

Here is the one I got involved in 1975 with my friend Richard Stallman, who is btw the reason why you have free software. In fact, your company is probably dependent on him. When I was beginning to be assistant professor at MIT, around 1973 to sometime later there, I was teaching introductory electrical circuits class some of the times and I observed that the teachers did not teach the students what they did. What teachers taught the students is lots of little equations, where you have to make equations for each of the parts, make equations for the way they are hooked together and you solve the equations. But unfortunately that is not what real electrical wizard does, so it was important to figure this out and write a program to do it. Part of my intention was to take the program that is written so clearly that I can give them to students and then they understand how to do it. Let me show you what I do when I see circuit like this ... (circuit analysis follows). I do not want to teach EE here, but I want to show you a method by which I am reasoning. I am using local information and propagate it around as far as I can. I am using the diagram as my memory, i.e. the diagram is finite number of places I can write things and if I am actually doing this job I would be writing down answers that I computed on each of these places, and I can trigger off stuff that is nearby. So that is a local analysis. If that was more complicated, there might be one simultaneous equation, like this feedback. Feedbak amplifier. One equation with one uknown. There might be two equations with two unknowns. That is the most horrible situation I can imagine. That's what real engineers do. 

### [Details appear from all directions](slides/details-appear-from-all-directions.png)

You are at home in your bedroom and elephant appears just in front of your bedroom door. Can elephant come into the bedroom with you? Why not? The reason is that the elephant is too big to get through the door. You look at the thing and say, my door is not big enough to do that. You are constructing a visual image, which is then being run by your visual system, and then being analyzed in such a way to produce the answer to this question. This is another example of such a thing. 

### [Not enough time!](slides/not-enough-time.png)

Here is really interesting thing. What is your name? That was 500 ms at most between when I finished the answer and he answered. Neuron time is about 5 or 10 ms. The fastest the neuron can do. That is few 10s of steps. I do not know how to make a computer that can do that ... I do not care how parallel the computer is (that is completely different question) ... how do I solve the problem of doing, phonology, morphology of words, the syntax, semantics and problem solving, back to the syntax, back to morphology of words, back to phonology, and then produce the muscle movements to answer the question, in 500 ms? There is not enough time. It means we do not know how to compute. That is one of the things that was bodering me for a while. Yes, our computers are billion times faster than that, step by step, but that does not help when you look at the problem like this. We really do not understand what we are doing which I think is very important. 

### [Propagation](slides/propagation.png)

I am going to tell you about a particular model of computation which is modelling what I am talking about. I am imagining a mechanism made out of a lot of little autonomous machines. Each machine is basically looking at neighbors, little cells (not biological) that connects to. Cells contain information. When information appears in some of the cells, a propagator might say, oh that is enough information for me to change something else. That is all. Nice property of the model is that it is scalable. Because everything is nice and local like this, it also turns out that it can avoid lots of the problems of parallelism when you are trying to simulate serial things with parallel. The hard thing is not making parallel machine, the hard thing is to simulate serial process on the parallel machine. If we realize that what we really want to do is write parallel processes in general, this problem mostly goes away. There is a breakthrough that makes this even more possible, by my graduate student Alexey Radul who did his doctor's thesis in 2009. In fact, his breakthrough is that values in the cells are not the values like 32. It could be, but they might be something like information, that something is greater than five and less then a hundred, partial information. Only thing that propagator can do is increase the content of information in the cell by restricting possible values that can be there. This has super wonderful property from parallel processing point of view that things eventually converge. It is eventual consistency feature. You can imagine things like ... I gave an example of numerical intervals, but unification patterns that people use in things like logic are perfectly plausible in such things as well. Also, algebraic expressions merge by equation solving. I do not know what to do about probability distributions. I do not understand probability, I can assure you of that. There has been of course this wonderful pile of work by all the wonderful Bayesians which has taken over the world for good reason. Edwin Thompson Jaynes is basically founder of that stuff and the other end of it is ... people who opposed him is Mr. Fisher of Fisher Information and they were fighting years before we got involved with computers, but remarkable thing is that they are both right, so you have to be careful - frequentists and Bayesians are right. They argue all they want, but they are asking different questions. But that is a whole different story. So the answer is, probability is pretty bad for a reason. Mr. Jayens in the second chapter of his book explains that logic is actually subtheory of probability, a specialization. Now, we know that logic is at least exponential in solving problems, SAT problems for example, so that is awful. Probability must be worse if you want to compute distributions because if we compute the probability of distributions we would get answers for the logic. So I cannot believe that that is an effective mechanism. On the other hand there are some very smart people, one of whom you may heard of, Vikash Sinha. He recently got his doctor's degree working for Josh Tanenbaum, where he approximates probability of distributions by sampling. A very elegant piece of work and I highly recommend reading about it. But I really do not understand probability. 

### [Example: Distance to M87](slides/example-distance-to-m87.png)

I am going to give an example which is purely about how to use propagators ... I am not going to say how they work inside because it is easy ... I highly recommend reading a paper by Alexey and me called [The art of the Propagator](http://web.mit.edu/~axch/www/art.pdf)  which basically explains how to write programs. In fact it truns out that the paper is also executable. This is also basically his Phd thesis that is written in the format that it can be loaded in an appropriate way to MIT Scheme system and it will just run. That is a literate programming style hack. I am going to tell something I know a lot about, which is astrophysics. How do you estimate a distance to a galaxy? This is giant galaxy in a Virgo cluster, M87, one of the largest galaxies we know of.

### [Propagation carries provenance](slides/propagation-carries-provenance.png)

I am going to make up some propagator cells. Specifically, I am gong to make up a cell for something called the distance modulus. You do not have to know anything about what these things mean. It is not relevant what they mean, I want you to see how things are hooked together. So, I have a distance modulus and a distance cell. Distance modulus is something that you measure by looking at the intensity of the light coming from this object and having an estimate of the actual absolute luminosity. ... Here is propagator called constraint propagator (c:mu<->d) which connects those two. I am wiring them together. Imagine these little machines being made. This propagator looking at these cells and saying ... um, is there anything happening. I am going to tell distance-modulus that the value is 31.4 +- .3 in some unit, and it has been told to us by VanDenBergh1985 when he took some images. He produced a big table you can find in literature. ... I can now ask what is M87:distance in Mpc. What you see immediately from here that I got this infromation by propagation from there. Provenance is being carried. 

### [Simple explanations](slides/simple-explanations.png)

I can ask various explanations, like "How do you know this?". Give me level-2 (very simple) explanation. It says a stupid thing, how it got it. 

### [More detail](slides/more-detail.png)

I can ask for more detailed information. Just to give you an idea. Now I am openning up these propagators. One of the most wonderful things about this kind of system which Stallman and I discovered way back in 1975 was, one of the best ways to use program that helps you to build electrical circuits or gives you partial analysis of the circuit was you can ask it: "How do you know that?". For example, it is possible that if I am designing one, I specify particular resistor ... suppose I have an amplifier and I want certain amount of output swing ... (follows description of an amplifier). Suppose that there is some contradiction in the design of an amplifier. I would get an argument from the computer - "the following constraints are in contradiction, here is the reason why I cannot work". We actually used that for teaching in some of our electric circuit classes. So getting an argument is a good thing. 

### [Information flows all ways](slides/information-flows-all-ways.png)

What we are seeing here is that John Tonry, who is a professor at University of Hawaii, has an estimate for distance ... If I ask what is the distance now, it actually contains 2 pieces of information because the interval produced by VanDenBergh overlaps the interval produced by Tonry. That overlap meant that there is a better estimate. Distance modulus has been also improved because the information that is increased in one of those cells simulated back propagation of information to the other.

### [More sources: Red Shifts](slides/more-sources-red-shifts.png)

We can talk about red shifts ...

### [Hubble Law](slides/hubble-law.png)

Now I can hook it together with Hubble's Law ...

### [Inconsistency and Multiple WorldViews](slides/inconsistency-and-multiple-worldviews.png)

There is even one more thing I am going to add ... There is contradiction between these measurements. Tonry measurement is incompatible with WMAP:1CDM assumption. That is very important because, first of all, at least one of these is wrong. Really important thing is what is truth if I believe one or the other? I should be able to modulate my understanding. I should be able to say, if I believe one set of assumptions, what's true?, if I believe the other set of assumptions, what would be the consequence. There might be programs which might try to resolve this by saying, what are the details on how these interact, is there somebody else who can give me information which will clarify this matter. So you have a program or a person whose job is to look at both points of view, to examine them and see how they fit.

Audience: There is a cycle in this lattice of nodes. How do you decide when to stop propagating information?  
GJS: Because information is monotonically increasing in the cell.  
Audience: So you reach a fixed point.  
GJS: Fixed point, that is right. If the cycle was something like the convergence of square root, it would take a lot of work. For things that I am showing you here, there are one or two steps.   

If I retract Tonry as being the source of information, maybe he was wrong, well then my estimate for my Hubble constant only depends upon WMAP3 and WMAP:1CDM. Whereas if I retract WMAP:1CDM and assert Tonry, then I get Hubble constant being dependent on Tonry and WMAP3. I can imagine a very important consequence of these nice things you can do with propagators, independent of everything else. As you probably know, Ioannidis wrote a paper why almost all medical papers are wrong. Very quick summary of it is, since papers in medical journals require confidence of 95% (1 in 20 experiments are wrong), and since everybody does hunderds of experiments and only publishes once which pass confidence measures, then almost everything in journals is wrong. It is very said. He is probably right. I have been on the advisory board of an organization that deals with multiple sclerosis because I had a student in my class who got it and asked me to be on this advisory board. I heard also horror stories about medical data being wrong for other reasons. Most medical data is collected in Excel spreadsheets. Now, most medical doctors have not a foggisest idea what the Excel spreadsheet actually does. It turns out there is an option with which you can sort a column without ?. Much of the data examined by my friends in multiple sclerosis foundation has been mangled in such a way that can only be explained by this. So there is a lot of very bad stuff out there. I believe we should have shot the guy who wrote Excel but that is not longer available.  

Audience: Presumably, you have all these different measurements, and they are linked behind the scenes by various formalisms.  
GJS: That is right. Imagine a spreadsheet, but one that has all different directions.   
Audience: There might be a contradiction ...  
GJS: You are saying that maybe formulas are wrong?   
Audience: Yes.  
GJS: Of course. To be perfectly honest, if we are doing this seriously, the formulas are premises, just like VanDenBergh, etc., but it would be too long to put on the slide. In fact I do it that way. I can attach those as well.   
Audience: You wind up with exhaustive ...  
GJS: Some of the things you want to be careful about, like in ordinary physics, do you really care whether F = ma being tentative. I would hope not. That was the question of presentation. What do you think are the delicate things of the data that you are invesigating and what are the once for which you can say, I will assume for the sake of argument are correct now?   

### [Dependencies in Logic: Suppes](slides/dependencies-in-logic.png)

I want to pop into talking about how do you do more general problem solving. What I have been showing you until now is plumbing, infrastructure. Type of things I have been talking to you about is mechanisms of shifting data around in a way you can imagine scaling at all levels, individual chips or even tens of thousands of little propagators on a chip and you can imagine them network wide as well because all they are are little independent machines sitting around, talking to their neighbors and asking, have you changed?, do you have some information for me?, can I change that one?. That is all it is happening. That has nothing to do with computer language either at this level. This kind of thinking can be done in any computer language and languages can interact because all that really matters is an API where propagators can talk to each other and maintain contents of cells, and merge procedures have to be universal. The important thing you saw there were dependencies. Dependencies are the information, the reason why believe something or not. It turns out that there is kind of logic ... there is a book "Introduction to Logic" by Patrick Suppes from Stanford, written maybe in the 50s, I am not sure, it is one of my favorite books, where he shows how to write rules of inference that allow you to follow arguments and keep the dependencies ... So, for example, here is modus ponens. Modus ponens is, if A implies B and A, then believe B. But, here is the story. A implies B has a name. It is statement number n, or statement name n, it does not matter. It is the identifier of that proposition. m is A, and o happens to be the name of the conclusion. This is the reason why I believe something. I left it out because I don't really care. I do not know why I believe n or m, but I believe them for some reason. I conclude B because I used modus ponens rule on n and m. But there is extra column which is very important. That is set of premises on which this depends. That is the thing like VanDenBergh, etc. The premise of n is d1, the premise of m is d2, and the union of those is the premise o depends upon. They are not always union, I have to careful about that. The deduction theorem, also known as conditional proof, which has the following rule of inference. It says, if I believe a premise called A (that is introduced as premise, hypothetical; it might not be true, it is just something I introduced), and B, then I can conclude A implies B by the conditional proof rule where dependencies ``d - (n)`` are reasons I believe it. That might not be obvious to you, but it is true. This rule shows that premises do not add up necessarily. So one thing that shows you is that once you have these propositions (and they have names), you got little more power. But I will tell you more, the power will improve. 

### [Propositions must have handles](slides/propositions-must-have-handles.png)

Suppose we are not talking about logic anymore, but useful rules, rule-based systems. I have "?A owes ?B the amount of money ?U", and the reason we believe that might be d1, etc. One way to solve this problem is ..., by split rule ... Unfortunately, this rule might not be useful in every situation. What if they are 1000 miles apart? However, they happen to be in the same restaurant. Then, this rule is suddenly useful. And reasons for believing this is the correct result does not depend on the fact you are in the restaurant. The argument might depend on it, what to tell people why it happened. But the ultimate premises do not containt restaurant part of the information. It should not. It does not matter whether this is the restaurant, this is still the right answer. Why is this important? This is imporant because much of the reasoning we do has to be specialized. It is very expensive to use modus ponens. It applies to anything. It is easy to make infinitely long deduction chains. But in very particular circumstances, it is appropriate to use it. For example, in the situation in which I have a proposition which might be very complicated, but I need that answer for some purpose, and I also have the implication that gets that in the right hand side, then it is probably worthwhile to investigate whether or not I can get the antecedent of that implication. I do not want to generally investigate it, I want to for that particular case. Similarly, so backward chaining is helpful in a sense. I am not going to do any work to solve the problem unless you need an answer to the problem. But the other way around is true too. I am not going to do any work trying to figure out what you need, unless I got enough evidence that it is worthwhile to do so, that I have some basis to start with. For example, backward chaining gets into horrible trouble producing very large number of sub-goals non of which are real, none of which can ever be satisfied because they might even be wrong. On the other hand, forward chaining can produce a large number of conclusions none of which is relevant to your problem. It is easy to generate those cases, even in little genealogies. 

### [States of Belief of Propositions](slides/states-of-belief-of-propositions.png)

Propositions have more than just true or false. This is not the multi-valued logic, that is not the goal. That is easy to simulate in ordinary logic. This is about having more power to say properties of rules that depend on the states of knowledge. I am not going to use words like true and false because I do not know what that means. That is relative to a problem, situation and sll sorts of things like that. It may be that from the point of view of some worldview, some assumptions, some premises, a proposition may be accepted. That is equivalent of true if I believe everything else that is relevant. Or maybe rejected - I know it is false. It could be known either if it is accepted or rejected or unknown if it is neither. And it could be a contradiction if it is both. Something is wrong if I believe something and it's negation. Those could be represented as cells in a propagator system and there are connections among them, like for example the ``or`` of acccepted and rejected is a known, the ``not`` of the known is unknown, and the ``and`` of accepted and rejected is constradictory, and they can be connected around, and rules start using these information. Now, what do I get out of this? 

### [Rules for States of Belief](slides/rules-for-states-of-belief.png)

I can write rules like this. This is the game of the restaurant thing. This is not written to be easy for people to read. I am sure we can write things that present this beautifully ... this is Lisp, this is what I like to write ... (follows description of the rule) ...

### [Handles and Dependencies -> Control](slides/handles-and-dependencies.png)

If I have handles, if I have name, a way of grabbing a proposition, and I have dependencies that say what that proposition depends upon, what premises (not the argument) that it depends upon, the I get control of the situation. Imagine how this works. These are propositions, "please-show ?b" - somebody wants the answer b. It was asserted. It is a piece of information. When this rule fires, g is bound to the acceptedness of that proposition, i.e. whoever is doing this deduction believes that. It will only fire if that acceptedness turns out to be true. In the context of this rule other rules are created. ... I am controlling the deductions being made by depending on thing I already know and what things I need, and these things are intervowen in some nice way. 

Audience: Is this the same system as in your astronomical example?  
GJS: Astronomical example showed you propagator system. This is showing you problem solver built in propagator system as the infrastructure. Propagator system is being used to represent these cells which are being manipulated to carry the dependencies.   
Audience: In you initial example, there was more than stating known and unknown, ranges ...  
GJS: Yes, of course. That can be true too. I am showing you an application.   
Audience: In that situation known and unknown are not used?  
GJS: In that one I did not tell you anything about known or unknown. That was purely numerical intervals being partial information structure.   
Audience: I wonder how are known and unknown as infrastrucure being used in that computation.   
GJS: They are not. This is about propositions. And this is a rule system with the infrastruture, sometimes the underlying library being a propagator system allowing the manipulation of unknowns and knowns and all that.   
Audience: You shown 2 examples and this is the machinery for the second example, is that right?    
GJS: I have shown you the example, I haven't shown you the machinery - this is by itself complicated. I have shown you none of the machinery. This is rule written for a rule-based system that is built on top of the propagator system.   

**fix this** The rule the ability to control forward chaining by needed result and control backward chaining by available facts. 

Audience: Analogous to Prolog in some way, you come to make sure your system ...    
GJS: No, it might take forever and Prolog often does and the reason is that Prolog is doing pure backward chaining with chronological backtracking (btw. this is doing smart backtracking, dependency-directed backtracking), is the same thing as in the SAT solver you would call that clausal learning, but that is a separate issue ... it is already under the table. The crucial thing going on here is that this is doing mostly foward reasoning, mostly antecedent reasoning, and the reason why it is doing that is because the statement I want to show something, is the statement that I might or might not know, I might now care about, but it is there ...  

### [So What](slides/so-what.png)

The idea I have shown you is plumbing, that is all it is. The nice thing about the plumbing is that, unlike logic, does not have ontological assumptions. Writing programs in Prolog is hell, to be perfectly honest. To understand what they are doing is hell. There are other things like rule-based things in general or ... you make things that are object-oriented ... now, it is very difficult to write something functionally that way. If you are writing functional program, it is hard to write object-oriented. There are all these barriers for thinking. You are stuck in one or the other kind of thinking, and my image of the way to do things is ... I am libertarian programmer. Maybe liberal politically, but libertarian programmer. Nobody is going to tell me how to program. If one of these propagators has to be written in assembly language, then it have to be. If the other one has to be written in Haskell, I will put up with the syntax. But it does not matter, the propagator idea is independent of language. Propagator model is basically parallel, and it is nice way to build constraint systems. It allows you to escape from expression-oriented mindset. That does not mean you cannot compile expression-oriented language into it. Alexey did for most of scheme. Chris and I are now trying to do a different way of doing that. It is very hard, but we have been at it only for a week. There is natural way to track provenance, because we are seeing appropriate tubing and generic operations happening there, carrying provenance around, as an example of metadata. There is many kinds of metadata which provenance is one. The nice thing I like is a way to work with locally consistent and globally inconsistent data, and there is SAT solver under the table, which is distrubuted. There is no little program called SAT solver, it is distributed thing which knows about contradictions and how to resolve them by getting rid of some hypothesis, basically hypothesis are labeled as hypothetical and ones which can be modulated in their truth.  

### Questions

Audience: I go back to the previous slide, you said we have a way of managing locally consistent and globally inconsistent data. What I saw in your example is mostly infrastructure which is able to claim and retract assertions, and you can juggle between which ones you want to believe, and see what happens if this or that set is true. Have you also built up a way to say, show me both consistent and inconsistent paths ...   
GJS: Yes and no, I have to be careful about that. I know how to do that, but particular system that we have does not. This is because it has single worldview in it, in any moment, there is time modulating it. But there are truth maintenance systems ... there is whole book about them by my former students Ken Forbus and Johan de Kleer which is called "Building Problem Solvers" which describes how to make TMSes in general of which one of the kind will allow you to look at all the alternatives ...    

Audience: You can say, this or that is true, or some of the assumptions or rule, like F = ma, and I am more confident in ones or the others. Have you incorporated confidence into this?  
GJS: No, I have done none of that.  
Audience: You said that you did not trust probability, you did not know how to ...   
GJS: Yes and no. The answer about probabilities is that I am afraid that once you have probability you will have computations that never converge. If the job is combining probability distributions, densities. If your job is estimating by sampling, which is what Vikash does, and you get some answer, like you get with Monte Carlo methods, but you cannot guarantee it.   

Audience: ?   
GJS: You are asking. what things were you tend to retract, if you find a contradiction? You would put the ordering on what premises you would mark as hypothetical. You could sort on that mark and therefore decide what to retract ... hypotheticaly.     

Audience: Some premises, if you move them, might destroy all of the other things you built, and others could ... But everything else stays fine ...   
GJS: Yes, that is why F = ma is pretty valuable. You are asking, can you estimate ahead of time? I do not know. I haven't done any investigation into estimating damage done by retracting a hypothesis.   

Audience: I would assume that when you are working with the system like this, provenance is of tremendeous value for debugging (GJS: Oh, yes), so ... If we divide it crudely there is 2 categories, computer did something that I did not expect (why did it do it?), and the computer did not do something that I expected it to do (why didn't it do it?). Provenance is great for the first, but for the second one, if you expected it to produce an answer, and it did not ... Is this provenance system helps you figure out why ...  
GJS: There is 2 parts to it. Part of it are dependencies, which are list of dependencies which something depends on. There is also the argument that you get by looking at the 3rd column in the Suppes's proofs. If it turned out that the reasons you do things ... depend upon things like that ... they might depend on "some mumble is unknown", then you can often look at that part of the answer which is a proof tree and see what is missing. That does not answer your question completely, but it gives you more evidence. In general, debugging is very hard, and we know that. I do not think that this solves the problem of debugging, but it does have the property that in many cases when something is broken ... I got a box something is wrong with it. That means, I put information on one end, the wrong thing comes at the other end. There are only 2 ways that can happen. Either one or more of the pieces inside that box is broken, or the way they are arranged is wrong at one level. As an engineer I got my scope and I measure input-output behavior of each of the subpieces, and if I find one of those is broken, then the problem is probably inside there, which means I want to find the smallest box that completely contains my problem. That one is going to have property that one of the pieces inside it are arranged wrong. That was theorem that was correctly proven, in my head. That is the only way I know really works for debugging - tracing by examining the pieces. Does that make sense?  
Audience: It makes sense. The thing I am worried about is ... using what did happen to figure out why something did not happen in the absense of more machinery seems to be explosive.  
GJS: Yes it is, but that is true in any kind of programming. What I am suggesting is, since I have in some sense a mechanical model in my mind (this is a wiring diagram - in fact the program to me is a textual description of an potentially infinite wiring diagram where recursion, etc. makes more parts) ... since it is a wiring diagram then I have techniques of putting in probes and seeing how inputs and outputs relate.  

Audience: Do you have any approaches of dealing with uncertainty?  
GJS: No. I do not have any magic formula. I think that probability, with high probability, is the best we can do. But, I think that the use of probability is very problematical. That is the difficulty.  

Audience: You have shown 2 examples in this presentation. How big of an example ...  
GJS: Your definition is like what happens at Google ... There was an example which is way to big to put on slides which is about hypothetical insurance company called ?, and this company is trying to decide whether or not people get insured based on their health habits. There are all sorts of things like, skiing is dangerous, skydiving is dangerous, eating crappy food is dangerous ... There are couple hunderd assertions associated with this, and the properties of the company is that they are very interested in having risk factors that can be accumulated so that if the risk is greater than mumble you get insurance, and if it is less you do, and if it somwhere in between it asks for more information. It is not going to look up on Facebook whether or not it can find pictures of you eating a hamburger unless it has information on other things. That kind of thing works beautifully. It is few hunderd assertions and about 10 pages of code describing Danny and Hal, etc. who all have bad habits.   
Audience: These assertions are hand-coded?  
GJS: Yes, I created this database by hand and created the rules. It was sort of a demo that I think is actually ... actually shows ... I was really trying to show how you can really avoid doing work by you not looking at something that is unknown unless you have reasons to investigate it. How do you accumulate reasons to investigate something.  

Audience: Something analogous to the hallucination ... people seem to ... that white including triangle. Can you demonstrate a propagator hallucinate?  
GJS: I am trying to get a graduate student work on that. I haven't found one who is gutsy enough. The idea is that you have some sort of "retina", and all over the place there are rules, that are little propagators that connect neighboring regions, and regions have multi-scale ?, and these guys talk to each other, etc., and whether or not it works, who knows. It is something like the Waltz's idea but at the scale where you fill your retina with constraints.  

Audience: Is there rule that says if you assume A and you get a contradiction, then accept not A, or reject A?  
GJS: Yes and no, because there might be many assumptions floating around. It is not just A. What if I assume A and B and C in various places, then I better do some very clever choice about what things to retract to avoid horrible exponential explosion that will occur. I assume that it will be exponential, but I what it to be much smaller.  
Audience: Maybe what the question is whether you have another network of stuff that will make decisions what you believe.  
GJS: Yes.  
Audience: ?  
GJS: No, you do not have to do something like that. You can do something much more complicated things. That is easy actually.  
Audience: ... if you accept B and C then you accept A.  
GJS: Yes, of course. To a great extent that is there anyway because the way clausal learning works which is the same thing as dependency-directed backtracking. You accumulate no-good sets, which are sets of assumptions which can some ? be true, such that one is smaller than the other, it subsumes the other, get rid of it, so that you do not have to look at it again, and that eliminates very large number of possibilitis for everything else. If these guys cannot together be true, then it does not matter what anything else is ... I clipped off whole piece of the search tree.  

Audience: You mentioned that this contains implicit SAT solver. ...  
GJS: This implementation is, like everything I got involved in, is very crude. I am after ideas. I am not going to sit and tweak the code. So, I am not using the best representations of sets. I am using lists that are linear perhaps, maybe not even sorted, which means intersections are square law. This is a demo, cause I am trying to investigate ideas. If I am going to do what you just said, yes, that is a year of work, and I would not learn very much. I might get somebody impressed, but I would not learn very much. And I care about learning. I am sure there is no problem in making very high performance of something like this.  

Audience: Is that going to work if you have any inputs rather than 2?  
GJS: Sure, that is easy. Thee is lots of stuff like that.  

Audience: You started by talking about psychology and example of asking Arthur his name. Then we stayed in the logical domain and set aside probability, and reason we set aside probability is because of doing any kind of sound probability is hard, but clearly we are taking (with our brain) in lots of noisy data, and in the noise there is contradiction, and we are doing some kind of sound but fast probabalistic reasoninig. We are throwing out outliers in the noise.  
GJS: I would avoid "probabalistic" ... there is some fast mechanism.  
Audience: Ok, there is some fast mechanism. We are doing it over noisy data that contains contradiction. We are throwing the outliers and forming pictures despite the outliers we throw out. Why not adopt some crude heuristics and admit some kind of very bad probability, with crude heuristics, so that the overall system can be more psychologically plausible.  
GJS: That is sometimes the right thing. Jake Beal did something like that. We were worried about how one might read text. However, people make very good things for that. How one would scan some text? You are looking at the sequence of letters, in straight text. You see not just letters, but little chunks. We create propagators whose job was to correlate. ... We wrote a paper on it, and it worked very well. But, that kind of thing can be easily done. The question is whether or not it has anything to do with psychology either. I do not know. I wanna be very careful about this. I do not believe anything that we call neurophysiology or psychology. That is because my friend Lettvin taught me that. One of the most important things he taugt me is this. That is: s = 1/2vt^2, right? That is how we describe it. It does know that. We have to be careful to separate our description of what we see things work from the mechanism. Mechanism of this we really do not know, although Einstein came close to making it clear. Behavior is easily described. Well, we observe smart people. Smart people seem to have some logic in them. Sometimes, they even work probabilistically. That is our description of what they do. I do not know how that works. Just think about how hard it is to understand how brain works. Way back in the 60s there was PDP-6 at MIT that I used a lot. We used to bring radios next to it and listen to ?, and every time Lisp GC will go (weird sound). That was mark-sweep. And things like that. But, basically we had things like that. There was a backplane full of hand-soldered connections. I actually ... So, there was a backplace. You could take your old oscilloscope. You could put probes on that and see spikes and so on. Now think about the following problem. Suppose I have infinite number of these. They are all slightly different. We have no diagrams. Big bus goes from here to here. You can throw darts at it, you can cut the wire, you might blow it away with the gun. OS fails in some interesing way. If you put a probe on it, you cannot put it back to the same place. You tell me that you understand it based on that kind of experiment. My feeling is that neurophysiology is going to give is some information in few hunderd years. The real value of it is medical. They do a lot of study so that they no how to fix people who are broken in some interesting way. But, exactly how machine works is just way beyond the technologies and mechanisms we have at this point. Even if I had a complete circuit diagram, it would not help very much. Look, we have a creature for which we have complete circuit diagram. It is a worm ?. We have every synapse between 100 or so neurons, I think. And no one can telly you, after studying for years, what that neuron does, or how it creates behavior that we see. This is tough.  
Audience: Just to be clear. ...  
GJS: The reason is because it is inspiring. We can see all sorts of things that we do not know how to do that people can do. And we can get into what kind of mechanism we as engineers can use that might get us that kind of behavior, whether or not that is how people do it.   

Audience: Your propagator diagram is basically a bunch of cells and wires, wiring them together, and you are building circuits this way.  
GJS: Right, they are little functional devices.  
Audience: And you have certain number of different kinds of wires.  
GJS: And you can dynamically build these things.  
Audience: Have you look at figuring out how to build systems differently ... you have a bunch of values in these cells, bunch of values in these. Tell me how things must be together and how these wires work.  
GJS: No, I have no idea. Tha is cool. One thing you can try, but this combinatorial disaster. You have all possible connections, This is something like what Waltz did. You have all possible connections, and you modulate the assumption that this one is there. That sounds to be unlikely to ever converge, for any reasonable dataset.  
Audience: ...  
GJS: ... the analysis actually ...  
Audience: Somebody figures out the relationship between volts, amps and wats. You can look at a bunch of data ...  
GJS: There is this character at Cornell who seems to be doing that by induction from numerical exmaples. I do not know how much credence to put into all those that have been published. I do not know how a hard problem can be solved. Remember, go into history of AI, there was this guy Doug Lenat who wrote a program called AM as his Phd thesis. AM was supposed to induce matematical truth from numerical examples. There was a self-limiting process, sort of a wind-up toy. I tried to tell him that at one point, it did not matter. The thing is that, it is easy to make those. It is very hard to tell, until you had run them for a while how much wind-up toy you have.  

Audience: ...  
GJS: You mean through the propagator network? Certainly it is the case that if these are really running in parallel, which you would hope, in a really nice implementation, and ... Redundant way of solving a problem is very important anyway.  
Audience: I am thinking about the triangle example. If you follow as certain way, you actually get an answer much faster ...  
GJS: I do not know how that triangle things works. It involves straight lines. That is very weird because there are no any straight lines in nature really, so this is not something you would expect to be built in. There is the amount of continuity there and I do not know how could this possibly evolve, but people from places where there are no straight lines at all, living in the middle of jungles, also see Kanizsa's triangle, so we know that this is not a cultural thing. 
Audience: They could see it on the horizon.  
GJS: If you live in a jungle you are not going to see a horizon anyway.  

Audience: Just to comment on this stuff ... psychologically plausible results ... the correct answer is never very interesing because you get it with some kind of convergent evolution. What is really interesting is to make a system that makes the same kind of mistakes as we do. ... the mistakes I make are uniquely human ...   
GJS: We can write programs that make mistakes too. Computers can make a billiion mistakes per second. I like to try to capture at least the way I like to thing as programs. It helps me to formalize it, and then I can try to debug myself. I like to write programs whenever I see I am making mistakes.  

Audience: I assume you are aware of Hewitt's "logic" work that he has been doing lately?  
GJS: I never understood Carl Hewitt, although I tried. I learned great deal from him because he is very, very smart. I have no idea what he is doing right now. I haven't seen him in years.  
Audience: I thought there might be some overlap ...  
GJS: There may be. He is impressive.  

Audience: Indepenently of AI or biological plausibility of how these things work, it is an interesting computational model, and I find myself wandering is it good way of organizing large programs.  
GJS: As an engineer that is what I am worried about. What I am mostly worried about and I teach a class every spring term is, how does one engineer systems or write programs that have the property that they can be used in ways not anticipated by the designer. How do you actually build flexibility in. I think that is very important. If the pressure is on making correct programs, then flexibility suffers. You have to pay certain amount of correctness for flexibility. I can talk about that some other time.  
Audience: Have you written anything else on that?  
GJS: I have written flames. Large, complicated flames with many, many pages.  

Audience: What do you teach in the class?  
GJS: It is called 6.945 and it will happen this spring.  
Chris: The old version is the way it was taught here at Google in 2009.  
GJS: There were major changes since then.  

Audience: You mentioned you have whole lot of stuff you could show us.  
GJS: The way I put this together is ... last night spent until 3 in the morning pasting together this PDF. 

#### Some related links:

[Rethinking AI: Constraints and Hallucinations Filling in the Details](http://cap.csail.mit.edu/sites/default/files/Sussman.pdf)
